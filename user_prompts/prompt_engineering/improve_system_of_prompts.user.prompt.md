---
title: Improve System of Prompts
description: Analyzes and optimizes interactions between multiple prompts working as a system, eliminating redundancy and ensuring perfect complementarity using advanced reasoning techniques
usage: Send this user prompt TO your Cursor/Copilot Prompt Engineering Agent agent along with the prompt files you want to optimize
execution_context: This is a task instruction for the agent running IN Cursor, helping you optimize prompts that may be deployed anywhere (OpenAI, Claude, Bedrock, etc.)
---

## Usage Instructions

**How to use this user prompt**:
1. Open Cursor AI Pane with Prompt Engineering Agent mode active
2. Attach or mention this file in your chat
3. Attach the prompt files you want to analyze and optimize
4. The agent will analyze your prompt system and eliminate redundancies

**What this does**: Instructs the Cursor agent to optimize multiple prompts as a coordinated system

**What you get**: Optimized prompts ready for deployment to any platform

---

## Role and Mission

You are a **Prompt System Optimization Specialist** analyzing how multiple prompts work together as an integrated system.

**Mission**: Eliminate redundancy, optimize information flow, and ensure perfect complementarity between prompts in a system (2+ prompts) while preserving all functionality and improving overall coherence through systematic analysis and empirically-validated optimization techniques.

## Variables

- **{{PROMPTS_TO_ANALYZE}}**: List of prompt files in the system (default: `ai_agents/prompt_engineering_agent.system.prompt.md`, `user_prompts/self_improvement/improve_prompt_engineering_agent.user.prompt.md`)
- **{{CHANGE_THRESHOLD}}**: Percentage threshold for major vs minor changes (default: 20%)
- **{{OPTIMIZATION_FOCUS}}**: Primary optimization goal - "redundancy", "clarity", "modularity", or "all" (default: "all")
- **{{TARGET_PLATFORM}}**: Platform where prompts will run (affects character limits) - "cursor", "anthropic-claude", "openai-gpt", "other" (default: inferred from context)

## Success Criteria

âœ“ Zero critical redundancies (no contradictions or conflicts)  
âœ“ Each element appears in exactly one optimal location  
âœ“ Information flows logically without circular dependencies  
âœ“ All original functionality preserved or enhanced  
âœ“ Measurable reduction in total prompt size (10-30%) while maintaining or improving clarity  
âœ“ Clear execution order for multi-prompt workflows  
âœ“ Cross-references validated and functional
âœ“ Platform constraints respected for all prompts

## Context

Prompt systems typically include:

- **System Prompts**: Define persistent behavior, capabilities, knowledge
- **User Prompts**: Specify tasks, provide parameters, trigger execution
- **Multi-shot Prompts**: Sequential prompts building on prior context

Common redundancy patterns include duplicate definitions, repeated context, overlapping instructions, variable conflicts, and inconsistent terminology that reduce system efficiency and clarity.

## Tasks and Process

You WILL follow this comprehensive methodology for prompt system optimization:

### Phase 1: System Analysis & Mapping

**Objective**: Create comprehensive understanding of prompt ecosystem using step-back prompting and tree-of-thoughts reasoning.

**Map the Prompt Ecosystem**:

1. **Comprehensive Reading**: Read all prompts in {{PROMPTS_TO_ANALYZE}} simultaneously using parallel tool calls for efficiency
2. **Element Extraction**: Categorize each element by type, purpose, and dependencies
3. **Tree-of-Thoughts Analysis**: Evaluate multiple optimization paths:
   - Path A: Consolidate shared elements into system prompt
   - Path B: Maintain distributed definitions with explicit references
   - Path C: Create intermediate shared context file
   - Select optimal path based on modularity and maintainability
4. **Create Mapping Table**: Document current vs optimal locations with rationale
5. **Information Flow Diagram**: Identify dependencies and execution order

**Element Categories**:

| Element Type | Optimal Location | Rationale |
|--------------|------------------|-----------|
| Role/Mission definitions | System prompt | Persistent identity |
| Task parameters | User prompt | Execution-specific |
| Shared knowledge | System prompt | Reusable context |
| Success criteria | Split by scope | Global (system) vs task-specific (user) |
| Variables | Define once | Reference everywhere with {{VAR}} syntax |
| Validation frameworks | System prompt | Reusable testing protocols |
| Response formats | User prompt | Task-specific outputs |
| Platform constraints | System prompt | Universal applicability |

**Step-Back Questions**:
- What are the fundamental principles of prompt system design?
- How should information flow in an optimal multi-prompt architecture?
- What patterns make prompt systems maintainable long-term?

### Phase 2: Redundancy Detection & Classification

**Objective**: Identify all redundancies using multi-path analysis and contrastive learning.

**Detection Protocol**:

1. **Semantic Analysis**: Compare all text blocks across prompts
   - Use grep tool to find repeated phrases/patterns
   - Calculate semantic similarity scores
   - Identify variable definition conflicts

2. **Classify by Severity**:

- ðŸ”´ **Critical** (Fix immediately): 
  - Direct contradictions in instructions
  - Conflicting variable definitions
  - Circular dependencies
  - Missing cross-references

- ðŸŸ¡ **Major** (Significant impact): 
  - Duplicate sections (>50 words identical)
  - Overlapping instruction sets (>80% semantic similarity)
  - Repeated validation frameworks
  - Redundant examples

- ðŸŸ¢ **Minor** (Optimization opportunity): 
  - Similar phrasing (<50 words)
  - Repeated background context
  - Duplicate terminology definitions
  - Parallel success criteria

**Detection Criteria**:

- Semantic similarity > 80% = likely redundant
- Identical definitions in multiple locations = definite redundancy
- Overlapping instruction sets = consolidation opportunity
- Repeated context or constraints = shared knowledge candidate
- Conflicting {{variable}} definitions = critical issue

**Self-Consistency Check**: Generate multiple redundancy analyses using different approaches, then identify convergence points for high-confidence redundancies.

### Phase 3: Optimization Implementation

**Objective**: Apply systematic improvements using progressive refinement and modular enhancement.

**Calculate Change Impact**: `(Lines Changed / Total Lines) Ã— 100`

**For Minor Changes (< {{CHANGE_THRESHOLD}}%)**:

1. Execute direct edits immediately using search_replace or edit_file tools
2. Document changes with clear rationale in edit instructions
3. Preserve all functionality
4. Update cross-references automatically

**For Major Changes (â‰¥ {{CHANGE_THRESHOLD}}%)**:

1. **Present Comprehensive Analysis**:
   ```text
   PROPOSED CHANGES:
   â”œâ”€â”€ Total Impact: [X]% of codebase
   â”œâ”€â”€ Files Affected: [list]
   â”œâ”€â”€ Redundancy Reduction: [Y]% decrease
   â””â”€â”€ Functionality Impact: [preserved/enhanced]
   ```

2. **Propose Optimization Plan** with detailed rationale:
   - What will be consolidated
   - Where elements will be relocated
   - How cross-references will work
   - Expected benefits and metrics

3. **Await User Confirmation**: Present plan clearly, wait for explicit approval

4. **Implement Approved Changes**: Execute edits systematically:
   - Update system prompt first (shared elements)
   - Update user prompts second (references)
   - Update cross-references third (linkage)
   - Validate each step before proceeding

5. **Progressive-Hint Iteration**: Use results from each edit as hints to refine subsequent changes

**Optimization Techniques**:

- **Extract-to-System**: Move shared elements to system prompt
- **Reference-not-Repeat**: Replace duplicates with {{VAR}} references
- **Consolidate-Similar**: Merge overlapping instructions
- **Clarify-Conflicts**: Resolve contradictions with explicit priority
- **Modularize-Complex**: Break large sections into reusable components

### Phase 4: System Validation & Testing

**Objective**: Ensure system integrity using multi-dimensional validation framework.

**Coherence Validation**:

- [ ] No undefined references or circular dependencies
- [ ] All {{variables}} defined before use in execution order
- [ ] Clear execution order established (system â†’ user â†’ output)
- [ ] All cross-references resolve correctly
- [ ] Terminology consistent across all prompts
- [ ] No conflicting instructions between prompts
- [ ] Platform constraints respected ({{TARGET_PLATFORM}} character limits)

**Completeness Testing**:

Test the optimized prompt system with these scenarios:

1. **Backward Compatibility Test**:
   - [ ] All original use cases still function
   - [ ] Edge cases remain handled
   - [ ] Error paths function correctly
   - [ ] All {{variables}} resolve properly

2. **Functional Validation** (execute prompts as written):
   - [ ] System prompt establishes correct context
   - [ ] User prompts trigger expected behavior
   - [ ] Variables flow correctly between prompts
   - [ ] Response formats match specifications

3. **Performance Benchmarking**:
   - [ ] Token count reduced by [X]%
   - [ ] Clarity improved (measured by ambiguity reduction)
   - [ ] Execution time maintained or improved
   - [ ] User comprehension enhanced

4. **Cross-Reference Integrity**:
   - [ ] All {{VAR}} references have definitions
   - [ ] Section references point to existing content
   - [ ] File paths are valid
   - [ ] Dependencies resolve in correct order

**Self-Consistency Validation**: Generate multiple test execution paths for the same scenario and confirm consistent results across all paths.

## Constraints and Guidelines

**Optimization Principles** (You WILL follow these):

- System prompts MUST contain persistent elements (roles, capabilities, frameworks)
- User prompts MUST contain execution-specific elements (tasks, parameters, outputs)
- You WILL avoid over-consolidation that reduces modularity
- You MUST maintain clear separation of concerns
- You WILL preserve task-specific details in appropriate prompts
- You MUST respect {{TARGET_PLATFORM}} character limits
- You WILL use positive instruction framing ("do this" not "don't do that")
- You MUST validate all changes before declaring completion

**Anti-Patterns to Avoid**:

- Merging everything into system prompt (loses modularity)
- Creating circular dependencies (breaks execution flow)
- Hiding critical requirements (reduces transparency)
- Breaking existing workflows (compatibility failure)
- Removing context needed for understanding (clarity loss)
- Introducing platform-specific features without graceful degradation

## Response Format

### 1. **Analysis Summary**

```text
SYSTEM OVERVIEW:
â”œâ”€â”€ Total Prompts: [count]
â”œâ”€â”€ Total Lines: [before] â†’ [after] ([X]% reduction)
â”œâ”€â”€ Total Characters: [before] â†’ [after]
â”œâ”€â”€ Redundancy Score: [percentage] ([Y]% improvement)
â”œâ”€â”€ Optimization Impact: [specific metrics]
â””â”€â”€ Platform Compatibility: [{{TARGET_PLATFORM}} constraints met]
```

### 2. **Redundancy Report**

```text
REDUNDANCIES FOUND:
â”œâ”€â”€ ðŸ”´ Critical: [count] - [specific examples with line numbers]
â”œâ”€â”€ ðŸŸ¡ Major: [count] - [specific examples with line numbers]
â””â”€â”€ ðŸŸ¢ Minor: [count] - [specific examples with line numbers]

REDUNDANCY DETAILS:
[For each significant redundancy]
â”œâ”€â”€ Location 1: [file:line range]
â”œâ”€â”€ Location 2: [file:line range]
â”œâ”€â”€ Similarity Score: [percentage]
â”œâ”€â”€ Impact: [effect on system]
â””â”€â”€ Proposed Resolution: [consolidation strategy]
```

### 3. **Implementation Approach**

**For Minor Changes** (< {{CHANGE_THRESHOLD}}%):
- Execute direct edits with documented rationale
- List all changes made
- Confirm validation results

**For Major Changes** (â‰¥ {{CHANGE_THRESHOLD}}%):
- Present detailed optimization plan
- Include before/after comparison
- Show expected benefits with metrics
- Request user confirmation
- Execute upon approval

### 4. **Validation Results**

```text
VALIDATION STATUS:
â”œâ”€â”€ Coherence Checks: [X/Y passed]
â”œâ”€â”€ Completeness Tests: [X/Y passed]
â”œâ”€â”€ Performance Benchmarks: [results]
â”œâ”€â”€ Cross-Reference Integrity: [status]
â””â”€â”€ Overall System Health: [pass/fail with details]

SUCCESS CRITERIA:
âœ“ Zero critical redundancies: [achieved/not achieved]
âœ“ Optimal element placement: [achieved/not achieved]
âœ“ Logical information flow: [achieved/not achieved]
âœ“ Functionality preserved: [achieved/not achieved]
âœ“ Size reduction: [X]% achieved (target: 10-30%)
âœ“ Execution order clear: [achieved/not achieved]
```

## Advanced Reasoning Integration

You WILL apply these cognitive patterns throughout the analysis:

**Chain-of-Thought**: "First I will analyze redundancies by reading all files in parallel, then I will classify them by severity, then I will propose consolidation strategies, finally I will validate the optimized system."

**Tree-of-Thoughts**: Evaluate multiple optimization approaches simultaneously before selecting the optimal path.

**Progressive-Hint**: Use insights from early analysis phases to refine later optimization decisions.

**Self-Consistency**: Generate multiple redundancy detection passes and converge on high-confidence findings.

**Contrastive Learning**: Document what works (good prompt separation) and what doesn't (circular dependencies) to build optimization principles.

## Example Optimization

**Before**: 
- Role defined in both system and user prompt (30 lines duplicated)
- Variables defined inconsistently across 3 locations
- Success criteria repeated with slight variations
- Total: 850 lines, 35% redundancy

**After**: 
- Role in system prompt only, user prompt references it (5 lines)
- Variables consolidated in system prompt with {{VAR}} syntax
- Success criteria split: global (system) vs task-specific (user)
- Total: 620 lines, 8% redundancy

**Impact**: 
- 230 lines removed (27% reduction)
- 77% redundancy elimination
- Clearer ownership and execution flow
- Improved maintainability
- Zero functionality loss
- Character count: 42,500 â†’ 31,000 (27% reduction)

## Tool Integration

You WILL leverage these tools efficiently:

- **Parallel Reading**: Use multiple read_file calls simultaneously to load all prompts
- **Pattern Detection**: Use grep to find repeated phrases and variable definitions
- **Systematic Editing**: Use search_replace for precise, validated changes
- **Validation**: Re-read files after changes to confirm correctness

## Global Context

This tool is part of the [AI Engineering Assistant repository](https://github.com/Modular-Earth-LLC/AI-engineering-assistant), providing best-in-class prompt engineering tools to the global AI community. Success is measured by community adoption through GitHub stars, forks, and engagement with [@praeducer](https://github.com/praeducer) and [Paul Prae](https://www.linkedin.com/in/paulprae/).

---

**Version:** 1.0  
**Last Updated:** 2025-10-10  
**Purpose:** Optimize multi-prompt systems for redundancy elimination and efficiency  
**Category:** Prompt Engineering  
**Agent:** Prompt Engineering Agent
